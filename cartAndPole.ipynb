{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend() # check if the backend is ipython\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep Q network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,img_height,img_width):\n",
    "        super().__init__() #call the constructor of the parent class\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3,out_features=24)\n",
    "        self.fc2 = nn.Linear(in_features=24,out_features=32)\n",
    "        self.out = nn.Linear(in_features=32,out_features=2)\n",
    "    \n",
    "    def forward(self,t): #forward pass of the neural network \n",
    "        t  = t.flatten(start_dim=1) \n",
    "        t = F.relu(self.fc1(t)) #pass the tensor through the first fully connected layer\n",
    "        t = F.relu(self.fc2(t)) #pass the tensor through the second fully connected layer\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experience class\n",
    "Experience = namedtuple('Experience',('state','action','next_state','reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Experience(2,3,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=2, action=3, next_state=1, reward=4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):#capacity is the maximum number of experiences that can be stored\n",
    "        self.capacity = capacity #how much experience can be stored\n",
    "        self.memory = []  #store the experience\n",
    "        self.push_count = 0 #keep track of past experiences\n",
    "        \n",
    "    def push(self,experience): #\n",
    "        if len(self.memory)< self.capacity: #if memory is not full\n",
    "            self.memory.append(experience)\n",
    "        else: # if memory is full ,it will overwrite the past experience\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1 #increment the push count by 1\n",
    "        \n",
    "    def sample(self, batch_size): #randomly sample the experience\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    def can_provide_sample(self, batch_size): #training is already started before the memeory is full\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStreategy():\n",
    "    def __init__(self, start , end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self,current_step):\n",
    "        return self.end + (self.start - self.end) *\\\n",
    "            math.exp(-1. * current_step * self.decay) #formula for epsilon greedy strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,strategy,num_actions,device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device #device is the hardware on which the model is running\n",
    "    \n",
    "    def select_action(self,state,policy_net):\n",
    "        rate = strategy.get_exploration_rate(self.current_step) \n",
    "        self.current_step += 1\n",
    "        \n",
    "        if rate > random.random():  #explore\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(device) \n",
    "        else:  #exploit\n",
    "            with torch.no_grad(): # to turn off gradient tracking\n",
    "                return policy_net(state).argmax(dim=1).to(device) #return the action with the highest Q value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class CartPoleEnvManager:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.env = gym.make('CartPole-v1', render_mode=\"rgb_array\")  # âœ… Correct render mode\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "        self.reset()  # âœ… Ensure reset initializes the environment correctly\n",
    "\n",
    "    def reset(self):\n",
    "        observation, _ = self.env.reset()  # âœ… Use [0] to get state in newer Gym versions\n",
    "        self.current_screen = None\n",
    "        return observation\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "    def take_action(self, action):\n",
    "        _, reward, self.done, _, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "\n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "\n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]  # âœ… Height is at index 2\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]  # âœ… Width is at index 3\n",
    "\n",
    "    def get_processed_screen(self):\n",
    "        try:\n",
    "            screen = self.env.render()\n",
    "            if screen is None:\n",
    "                print(\"âš ï¸ `render()` returned None. Trying `render_frame()`...\")\n",
    "                screen = self.env.render_frame()  # âœ… Use this if `render()` fails\n",
    "        except AttributeError:\n",
    "            print(\"ðŸš¨ `render_frame()` not available. Check Gym version.\")\n",
    "\n",
    "        if screen is None:\n",
    "            raise ValueError(\"ðŸš¨ Still getting None for screen rendering. Check Gym settings.\")\n",
    "\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen).permute(2, 0, 1)\n",
    "        \n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((40, 90)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Hyperparameters ===\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lt = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "# === Initialize Environment & Components ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em = CartPoleEnvManager(device)\n",
    "\n",
    "# âœ… Fix typo: Correct `EpsilonGreedyStrategy`\n",
    "strategy = EpsilonGreedyStreategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "# âœ… Ensure `Agent` class is initialized correctly\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "# âœ… Ensure `DQN` is initialized with correct input sizes\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "\n",
    "# âœ… Sync weights & set evaluation mode\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# âœ… Fix learning rate parameter name (`lr=lt`)\n",
    "optimizer = torch.optim.Adam(params=policy_net.parameters(), lr=lt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gow20\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\gow20\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m dqn(state)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Run the environment with visualization\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Updated unpacking\u001b[39;00m\n\u001b[0;32m     42\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     44\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Define the Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 24)\n",
    "        self.fc2 = nn.Linear(24, 32)\n",
    "        self.out = nn.Linear(32, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Set up the CartPole environment\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "dqn = DQN(state_dim, action_dim)\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=0.001)\n",
    "memory = deque(maxlen=10000)\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return dqn(state).argmax(dim=1).view(1, 1)\n",
    "\n",
    "# Run the environment with visualization\n",
    "state, _ = env.reset()  # Updated unpacking\n",
    "state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = select_action(state, epsilon=0.1)\n",
    "    next_state, reward, done, _, _ = env.step(action.item())  # Updated unpacking\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
